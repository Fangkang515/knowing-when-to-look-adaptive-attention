{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from scipy.misc import imread, imresize\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "from PIL import Image\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'BEST_checkpoint_10.pth.tar'\n",
    "checkpoint = torch.load(checkpoint)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "with open('WORDMAP.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "rev_word_map = {v: k for k, v in word_map.items()}  # idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(image): \n",
    "    \"\"\"\n",
    "    Predict output with beam size of 1 (predict the word and feed it to the next LSTM). \n",
    "    Prints out the generated sentence\n",
    "    \"\"\"\n",
    "    max_len = 20\n",
    "    begining_lstm = True\n",
    "    begining_sen = True\n",
    "\n",
    "    sampled = []\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}  # idx2word\n",
    "\n",
    "    img = imread(image)\n",
    "    img = imresize(img, (254, 254))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = img / 255.\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 254, 254)\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 254, 254)\n",
    "    spatial_image, global_image, enc_image = encoder(image)\n",
    "    pred = torch.LongTensor([[word_map['<start>']]]).to(device)   # (1, 1)  \n",
    "    hidden, cell = decoder.init_hidden_state(enc_image)       #  (1,hidden_size)\n",
    "    num_pix = spatial_image.shape[1]\n",
    "    alphas_stored = torch.zeros(max_len, num_pix)\n",
    "    betas_stored = torch.zeros(max_len,1)\n",
    "\n",
    "    for timestep in range(max_len):\n",
    "        embeddings = decoder.embedding(pred).squeeze(1)       # (1,1,embed_dim) --> (1,embed_dim)    \n",
    "        inputs = torch.cat((embeddings,global_image), dim = 1)    # (1, embed_dim * 2)\n",
    "        if begining_lstm:\n",
    "            h, c = decoder.LSTM(inputs, (hidden, cell))     # (1, hidden_size)\n",
    "            begining_lstm = False\n",
    "        else:  \n",
    "            h_prev = h.clone()\n",
    "            c_prev = c.clone()\n",
    "            h, c = decoder.LSTM(inputs, (h, c))  # (1, hidden_size)\n",
    "        # Run the sentinal model\n",
    "        if begining_sen:\n",
    "            st = decoder.sentinal(inputs, hidden, cell)\n",
    "            begining_sen = False\n",
    "        else:\n",
    "            st = decoder.sentinal(inputs, h_prev, c_prev)\n",
    "\n",
    "        alpha_t, ct, zt = decoder.spatial_attention(spatial_image,h)\n",
    "        # Run the adaptive attention model\n",
    "        c_hat, beta_t, alpha_hat = decoder.adaptive_attention(h, st, zt, ct)\n",
    "        # Compute the probability\n",
    "        pt = decoder.fc(c_hat + h)  \n",
    "        _,pred = pt.max(1)\n",
    "        sampled.append(pred.item())\n",
    "        alphas_stored[timestep] = alpha_t\n",
    "        betas_stored[timestep] = beta_t\n",
    "\n",
    "    generated_words = [rev_word_map[sampled[i]] for i in range(len(sampled))]\n",
    "    filtered_words = ' '.join([word for word in generated_words if word != '<end>'])\n",
    "\n",
    "    print(\"Prediction Using Greedy Search:\", filtered_words)\n",
    "    print(\"Betas:\", betas_stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation with Beam Search\n",
    "def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n",
    "    \n",
    "    k = beam_size\n",
    "    vocab_size = len(word_map)\n",
    "\n",
    "    # Read image and process\n",
    "    img = imread(image_path)\n",
    "    if len(img.shape) == 2:\n",
    "        img = img[:, :, np.newaxis]\n",
    "        img = np.concatenate([img, img, img], axis=2)\n",
    "    img = imresize(img, (254, 254))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = img / 255.\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 254, 254)\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 254, 254)\n",
    "    spatial_image, global_image, encoder_out = encoder(image) #enc_image of shape (batch_size,num_pixels,features)\n",
    "    # Flatten encoding\n",
    "    num_pixels = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(2)\n",
    "    enc_image_size = 8     \n",
    "    # We'll treat the problem as having a batch size of k\n",
    "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "    # Tensor to store top k previous words at each step; now they're just <start>\n",
    "    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "    # Tensor to store top k sequences; now they're just <start>\n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
    "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
    "    #Tensor to store the top k sequences betas\n",
    "    seqs_betas = torch.ones(k,1,1).to(device) \n",
    "    # Lists to store completed sequences, their alphas, betas and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_alpha = list()\n",
    "    complete_seqs_scores = list()\n",
    "    complete_seqs_betas = list()       \n",
    "    # Start decoding\n",
    "    step = 1\n",
    "    h, c = decoder.init_hidden_state(encoder_out)\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "    while True:\n",
    "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  \n",
    "        inputs = torch.cat((embeddings,global_image.expand_as(embeddings)), dim = 1)\n",
    "        h_prev = h.clone()\n",
    "        c_prev = c.clone()\n",
    "        h, c = decoder.LSTM(inputs, (h, c))  # (1, hidden_size)\n",
    "        # Run the sentinal model\n",
    "        st = decoder.sentinal(inputs, h_prev, c_prev)\n",
    "        alpha, ct, zt = decoder.spatial_attention(spatial_image,h)\n",
    "        # Run the adaptive attention model\n",
    "        c_hat, beta_t, alpha_hat = decoder.adaptive_attention(h, st, zt, ct)\n",
    "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
    "        # Compute the probability\n",
    "        scores = decoder.fc(c_hat + h) \n",
    "        scores = F.log_softmax(scores, dim=1)   # (s, vocab_size)\n",
    "        # Add\n",
    "        # (k,1) will be (k,vocab_size), then (k,vocab_size) + (s,vocab_size) --> (s, vocab_size)\n",
    "        scores = top_k_scores.expand_as(scores) + scores  \n",
    "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            #Remember: torch.topk returns the top k scores in the first argument, and their respective indices in the second argument\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words / vocab_size  # (s) \n",
    "        next_word_inds = top_k_words % vocab_size  # (s) \n",
    "        # Add new words to sequences, alphas\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "        # (s, step+1, enc_image_size, enc_image_size)\n",
    "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],dim=1)  \n",
    "        seqs_betas = torch.cat([seqs_betas[prev_word_inds], beta_t[prev_word_inds].unsqueeze(1)], dim=1)  \n",
    "\n",
    "        # Which sequences are incomplete (didn't reach <end>)?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map['<end>']]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # Set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            complete_seqs_betas.extend(seqs_betas[complete_inds])   \n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # Proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "        seqs = seqs[incomplete_inds]              \n",
    "        seqs_alpha = seqs_alpha[incomplete_inds]   \n",
    "        seqs_betas = seqs_betas[incomplete_inds]    \n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "    seq = complete_seqs[i]\n",
    "    alphas = complete_seqs_alpha[i]\n",
    "    betas = complete_seqs_betas[i]          \n",
    "\n",
    "    return seq, alphas, betas     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_att(image_path, seq, alphas, betas, rev_word_map, smooth=True):\n",
    "    \"\"\"\n",
    "    Visualizes caption with weights at every word.\n",
    "    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n",
    "    :param image_path: path to image that has been captioned\n",
    "    :param seq: caption\n",
    "    :param alphas: weights\n",
    "    :param rev_word_map: reverse word mapping, i.e. ix2word\n",
    "    :param smooth: smooth weights?\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([8 * 8, 8 * 8], Image.LANCZOS)\n",
    "    words = [rev_word_map[ind] for ind in seq]\n",
    "    print(' '.join(words[1:-1]))\n",
    "\n",
    "    for t in range(len(words)):\n",
    "        if t > 50:\n",
    "            break\n",
    "        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n",
    "        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n",
    "        plt.text(50, 1, '%.3f' % (betas[t].item()), color='red', backgroundcolor='white', fontsize=10)\n",
    "        plt.imshow(image)\n",
    "        current_alpha = alphas[t, :]\n",
    "        if smooth:\n",
    "            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=8, sigma=8)\n",
    "        else:\n",
    "            alpha = skimage.transform.resize(current_alpha.numpy(), [8 * 8, 8 * 8])\n",
    "        if t == 0:\n",
    "            plt.imshow(alpha, alpha=0)\n",
    "        else:\n",
    "            plt.imshow(alpha, alpha=0.8)\n",
    "        plt.set_cmap(cm.Greys_r)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_output('test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 12)  # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "# Encode, decode with attention and beam search k=3\n",
    "seq, alphas, betas = caption_image_beam_search(encoder, decoder, 'test.jpg', word_map)\n",
    "alphas = torch.FloatTensor(alphas)\n",
    "# Visualize caption and attention of best sequence\n",
    "visualize_att('test.jpg', seq, alphas, betas, rev_word_map, smooth=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
