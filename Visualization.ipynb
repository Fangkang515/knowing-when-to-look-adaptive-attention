{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Knowing When to Look: Adaptive Attention via a Visual Sentinal for Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from scipy.misc import imread, imresize\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "from PIL import Image\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint = 'BEST_checkpoint_17.pth.tar' \n",
    "checkpoint = torch.load(checkpoint)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "with open('WORDMAP.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "rev_word_map = {v: k for k, v in word_map.items()}  # idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(image): \n",
    "    \"\"\"\n",
    "    Predict output with beam size of 1 (predict the word and feed it to the next LSTM). \n",
    "    Prints out the generated sentence\n",
    "    \"\"\"\n",
    "    max_len = 20\n",
    "    begining_lstm = True\n",
    "    begining_sen = True\n",
    "\n",
    "    sampled = []\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}  # idx2word\n",
    "\n",
    "    img = imread(image)\n",
    "    img = imresize(img, (256, 256))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = img / 255.\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 256, 256)\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
    "    spatial_image, global_image, enc_image = encoder(image)\n",
    "    pred = torch.LongTensor([[word_map['<start>']]]).to(device)   # (1, 1)  \n",
    "    hidden, cell = decoder.init_hidden_state(enc_image)       #  (1,hidden_size)\n",
    "    num_pix = spatial_image.shape[1]\n",
    "    alphas_stored = torch.zeros(max_len, num_pix)\n",
    "\n",
    "    for timestep in range(max_len):\n",
    "        embeddings = decoder.embedding(pred).squeeze(1)       # (1,1,embed_dim) --> (1,embed_dim)    \n",
    "        inputs = torch.cat((embeddings,global_image), dim = 1)    # (1, embed_dim * 2)\n",
    "        if begining_lstm:\n",
    "            h, c = decoder.LSTM(inputs, (hidden, cell))     # (1, hidden_size)\n",
    "            begining_lstm = False\n",
    "        else:  \n",
    "            h, c = decoder.LSTM(inputs, (h, c))  # (1, hidden_size)\n",
    "        # Run the sentinal model\n",
    "        if begining_sen:\n",
    "            st = decoder.sentinal(inputs, hidden, cell)\n",
    "            begining_sen = False\n",
    "        else:\n",
    "            st = decoder.sentinal(inputs, h, c)\n",
    "\n",
    "        alpha_t, ct, zt = decoder.spatial_attention(spatial_image,h)\n",
    "        # Run the adaptive attention model\n",
    "        c_hat, beta_t, alpha_hat = decoder.adaptive_attention(h, st, zt, ct)\n",
    "        # Compute the probability\n",
    "        pt = decoder.fc(c_hat + h)  \n",
    "        _,pred = pt.max(1)\n",
    "        sampled.append(pred.item())\n",
    "        alphas_stored[timestep] = alpha_t\n",
    "\n",
    "    generated_words = [rev_word_map[sampled[i]] for i in range(len(sampled))]\n",
    "    filtered_words = ' '.join([word for word in generated_words if word != '<end>'])\n",
    "    print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation with Beam Search\n",
    "def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n",
    "    \n",
    "    k = beam_size\n",
    "    vocab_size = len(word_map)\n",
    "\n",
    "    # Read image and process\n",
    "    img = imread(image_path)\n",
    "    if len(img.shape) == 2:\n",
    "        img = img[:, :, np.newaxis]\n",
    "        img = np.concatenate([img, img, img], axis=2)\n",
    "    img = imresize(img, (256, 256))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = img / 255.\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 256, 256)\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
    "    spatial_image, global_image, encoder_out = encoder(image) #enc_image of shape (batch_size,num_pixels,features)\n",
    "    # Flatten encoding\n",
    "    num_pixels = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(2)\n",
    "    enc_image_size = 8         # change to 7 if resnet output is 7x7 (if using 254x254 image)\n",
    "    # We'll treat the problem as having a batch size of k\n",
    "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "    # Tensor to store top k previous words at each step; now they're just <start>\n",
    "    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "    # Tensor to store top k sequences; now they're just <start>\n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
    "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
    "    # Lists to store completed sequences, their alphas and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_alpha = list()\n",
    "    complete_seqs_scores = list()\n",
    "    # Start decoding\n",
    "    step = 1\n",
    "    h, c = decoder.init_hidden_state(encoder_out)\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "    while True:\n",
    "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  \n",
    "        inputs = torch.cat((embeddings,global_image.expand_as(embeddings)), dim = 1)    \n",
    "        h, c = decoder.LSTM(inputs, (h, c))  # (1, hidden_size)\n",
    "        # Run the sentinal model\n",
    "        st = decoder.sentinal(inputs, h, c)\n",
    "        alpha, ct, zt = decoder.spatial_attention(spatial_image,h)\n",
    "        # Run the adaptive attention model\n",
    "        c_hat, beta_t, alpha_hat = decoder.adaptive_attention(h, st, zt, ct)\n",
    "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
    "        # Compute the probability\n",
    "        scores = decoder.fc(c_hat + h) \n",
    "        scores = F.log_softmax(scores, dim=1)   # (s, vocab_size)\n",
    "        # Add\n",
    "        # (k,1) will be (k,vocab_size), then (k,vocab_size) + (s,vocab_size) --> (s, vocab_size)\n",
    "        scores = top_k_scores.expand_as(scores) + scores  \n",
    "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            #Remember: torch.topk returns the top k scores in the first argument, and their respective indices in the second argument\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words / vocab_size  # (s) \n",
    "        next_word_inds = top_k_words % vocab_size  # (s) \n",
    "        # Add new words to sequences, alphas\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "        # (s, step+1, enc_image_size, enc_image_size)\n",
    "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],dim=1)  \n",
    "\n",
    "        # Which sequences are incomplete (didn't reach <end>)?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map['<end>']]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # Set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # Proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "        seqs = seqs[incomplete_inds]              \n",
    "        seqs_alpha = seqs_alpha[incomplete_inds]   \n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "    seq = complete_seqs[i]\n",
    "    alphas = complete_seqs_alpha[i]\n",
    "\n",
    "    return seq, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_att(image_path, seq, alphas, rev_word_map, smooth=True):\n",
    "    \"\"\"\n",
    "    Visualizes caption with weights at every word.\n",
    "    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n",
    "    :param image_path: path to image that has been captioned\n",
    "    :param seq: caption\n",
    "    :param alphas: weights\n",
    "    :param rev_word_map: reverse word mapping, i.e. ix2word\n",
    "    :param smooth: smooth weights?\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([8 * 8, 8 * 8], Image.LANCZOS)\n",
    "    words = [rev_word_map[ind] for ind in seq]\n",
    "    print(' '.join(words[1:-1]))\n",
    "\n",
    "    for t in range(len(words)):\n",
    "        if t > 50:\n",
    "            break\n",
    "        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n",
    "        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n",
    "        plt.imshow(image)\n",
    "        current_alpha = alphas[t, :]\n",
    "        if smooth:\n",
    "            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=8, sigma=8)\n",
    "        else:\n",
    "            alpha = skimage.transform.resize(current_alpha.numpy(), [8 * 8, 8 * 8])\n",
    "        if t == 0:\n",
    "            plt.imshow(alpha, alpha=0)\n",
    "        else:\n",
    "            plt.imshow(alpha, alpha=0.8)\n",
    "        plt.set_cmap(cm.Greys_r)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_output('test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode, decode with attention and beam search k=3\n",
    "seq, alphas = caption_image_beam_search(encoder, decoder, 'test.jpg', word_map)\n",
    "alphas = torch.FloatTensor(alphas)\n",
    "# Visualize caption and attention of best sequence\n",
    "visualize_att('test.jpg', seq, alphas, rev_word_map, smooth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
